{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **FOLDER IMPLEMANTATION**"
      ],
      "metadata": {
        "id": "iFJa8gEN-j-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2  # OpenCV for image processing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to train the perceptron\n",
        "def trainPerceptron(inputs, t, weights, rho, iterNo):\n",
        "    # Shuffle the inputs and labels\n",
        "    inputs, t = shuffle(inputs, t)\n",
        "\n",
        "    # Add bias to inputs\n",
        "    bias = np.ones((inputs.shape[0], 1))  # Create a bias term\n",
        "    inputs = np.hstack((bias, inputs))  # Add bias to the input vectors\n",
        "\n",
        "    # Training the perceptron\n",
        "    for _ in range(iterNo):\n",
        "        for i in range(inputs.shape[0]):\n",
        "            # Compute the output (activation)\n",
        "            activation = np.dot(inputs[i], weights)\n",
        "            # Activation function: step function\n",
        "            prediction = 1 if activation >= 0 else 0\n",
        "\n",
        "            # Update weights if prediction does not match the target\n",
        "            if prediction != t[i]:\n",
        "                weights += rho * (t[i] - prediction) * inputs[i]\n",
        "\n",
        "    # Save the trained weights\n",
        "    np.save('weights.npy', weights)\n",
        "    return weights\n",
        "\n",
        "# Function to test the perceptron\n",
        "def testPerceptron(sample_test, weights):\n",
        "    # Load weights\n",
        "    weights = np.load('weights.npy')\n",
        "\n",
        "    # Add bias to the test sample\n",
        "    bias = np.array([1])  # Create a bias term\n",
        "    sample_test = np.hstack((bias, sample_test))  # Add bias to the test vector\n",
        "\n",
        "    # Compute the output (activation)\n",
        "    activation = np.dot(sample_test, weights)\n",
        "    # Activation function: step function\n",
        "    prediction = 1 if activation >= 0 else 0\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Load training data\n",
        "def load_data(train_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_folder in os.listdir(train_dir):\n",
        "        class_path = os.path.join(train_dir, class_folder)\n",
        "        for image_file in os.listdir(class_path):\n",
        "            img = cv2.imread(os.path.join(class_path, image_file))\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, (224, 224))  # Resize image as necessary\n",
        "                images.append(img.flatten())  # Flatten the image into a vector\n",
        "                labels.append(class_folder)  # Use folder name as label\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    inputs = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    t = label_encoder.fit_transform(labels)  # Convert class names to integers\n",
        "\n",
        "    return inputs, t, label_encoder\n",
        "\n",
        "# Define paths\n",
        "train_dir = '/content/train'\n",
        "test_dir = '/content/test'\n",
        "\n",
        "# Load training data\n",
        "inputs, t, label_encoder = load_data(train_dir)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(inputs, t, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize weights randomly\n",
        "weights = np.random.rand(X_train.shape[1] + 1)  # +1 for bias\n",
        "\n",
        "# Define learning rate and iterations\n",
        "rho = 0.1\n",
        "iterNo = 1000\n",
        "\n",
        "# Train the perceptron\n",
        "weights = trainPerceptron(X_train, y_train, weights, rho, iterNo)\n",
        "\n",
        "# Load testing data\n",
        "test_images = []\n",
        "true_test_labels = []\n",
        "\n",
        "for class_folder in os.listdir(test_dir):\n",
        "    class_path = os.path.join(test_dir, class_folder)\n",
        "    for image_file in os.listdir(class_path):\n",
        "        img = cv2.imread(os.path.join(class_path, image_file))\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (224, 224))  # Resize image as necessary\n",
        "            test_images.append(img.flatten())  # Flatten the image into a vector\n",
        "            true_test_labels.append(class_folder)  # Use folder name as label\n",
        "\n",
        "# Convert test images to numpy array\n",
        "sample_tests = np.array(test_images)\n",
        "\n",
        "# Test the perceptron on test data and calculate accuracy\n",
        "correct_predictions = 0\n",
        "for sample_test, true_label in zip(sample_tests, true_test_labels):\n",
        "    prediction = testPerceptron(sample_test, weights)\n",
        "    predicted_class = label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "\n",
        "    if predicted_class == true_label:\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / len(sample_tests)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7HgrIvv-jLP",
        "outputId": "e757b559-1e18-42da-beb6-79fa7aa996fa"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 42.11%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MORE ACCURACY**"
      ],
      "metadata": {
        "id": "zcRunZ-RkKNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2  # OpenCV for image processing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Veri yükleme fonksiyonu\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for class_folder in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_folder)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        for image_file in os.listdir(class_path):\n",
        "            image_path = os.path.join(class_path, image_file)\n",
        "            img = cv2.imread(image_path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, (64, 64))  # Boyutu küçültüyoruz\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Gri tonlamaya çeviriyoruz\n",
        "                images.append(img.flatten())  # Görüntüyü vektöre dönüştürüyoruz\n",
        "                labels.append(class_folder)  # Klasör ismini etiket olarak kullanıyoruz\n",
        "\n",
        "    # NumPy dizilerine dönüştürüyoruz\n",
        "    inputs = np.array(images, dtype=np.float32)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "# Aktivasyon fonksiyonu (Softmax)\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Sayısal kararlılık için\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Kayıp fonksiyonu (Cross-Entropy)\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    loss = -np.sum(y_true * np.log(y_pred)) / m\n",
        "    return loss\n",
        "\n",
        "# Tahmin fonksiyonu\n",
        "def predict(X, weights):\n",
        "    z = np.dot(X, weights)\n",
        "    y_pred = softmax(z)\n",
        "    return y_pred\n",
        "\n",
        "# Eğitim fonksiyonu\n",
        "def train_perceptron(X_train, y_train, num_classes, learning_rate=0.01, epochs=2000):\n",
        "    num_samples, num_features = X_train.shape\n",
        "    weights = np.random.randn(num_features, num_classes) * 0.01\n",
        "\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        z = np.dot(X_train, weights)\n",
        "        y_pred = softmax(z)\n",
        "        loss = cross_entropy(y_pred, y_train)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        error = y_pred - y_train\n",
        "        gradient = np.dot(X_train.T, error) / num_samples\n",
        "        weights -= learning_rate * gradient\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}')\n",
        "\n",
        "    return weights, loss_history\n",
        "\n",
        "# Veri yolları\n",
        "train_dir = '/content/train'\n",
        "test_dir = '/content/test'\n",
        "\n",
        "# Eğitim verilerini yüklüyoruz\n",
        "inputs, labels = load_data(train_dir)\n",
        "\n",
        "# Etiketleri encode ediyoruz\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# One-hot encoding\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "labels_one_hot = one_hot_encoder.fit_transform(labels_encoded.reshape(-1, 1))\n",
        "\n",
        "# Verileri karıştırıyoruz\n",
        "inputs, labels_one_hot = shuffle(inputs, labels_one_hot, random_state=42)\n",
        "\n",
        "# Veriyi eğitim ve doğrulama setlerine bölüyoruz\n",
        "X_train, X_val, y_train, y_val = train_test_split(inputs, labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Özellikleri ölçeklendiriyoruz\n",
        "X_train = X_train / 255.0\n",
        "X_val = X_val / 255.0\n",
        "\n",
        "# Bias terimini ekliyoruz\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
        "X_val = np.hstack([np.ones((X_val.shape[0], 1)), X_val])\n",
        "\n",
        "# Modeli eğitiyoruz\n",
        "weights, loss_history = train_perceptron(X_train, y_train, num_classes, learning_rate=0.001, epochs=2000)\n",
        "\n",
        "# Ağırlıkları kaydediyoruz\n",
        "np.save('weights2.npy', weights)\n",
        "\n",
        "# Ağırlıkları yüklüyoruz\n",
        "weights = np.load('weights2.npy')\n",
        "\n",
        "# Test verilerini yüklüyoruz\n",
        "test_inputs, test_labels = load_data(test_dir)\n",
        "test_inputs = test_inputs / 255.0  # Özellikleri ölçeklendiriyoruz\n",
        "test_inputs = np.hstack([np.ones((test_inputs.shape[0], 1)), test_inputs])  # Bias terimini ekliyoruz\n",
        "\n",
        "# Tahmin yapıyoruz\n",
        "y_pred = predict(test_inputs, weights)\n",
        "predicted_classes = label_encoder.inverse_transform(np.argmax(y_pred, axis=1))\n",
        "\n",
        "# Gerçek etiketleri alıyoruz\n",
        "true_classes = test_labels\n",
        "\n",
        "# Doğruluğu hesaplıyoruz\n",
        "correct_predictions = 0\n",
        "for predicted_class, true_class in zip(predicted_classes, true_classes):\n",
        "\n",
        "    if predicted_class == true_class:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_inputs)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ft9SUfAg7JD",
        "outputId": "d5d7d5a3-30a8-4cc7-93d1-b0fdbec3ddd1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/2000, Loss: 0.8402\n",
            "Epoch 200/2000, Loss: 0.7554\n",
            "Epoch 300/2000, Loss: 0.6982\n",
            "Epoch 400/2000, Loss: 0.6520\n",
            "Epoch 500/2000, Loss: 0.6123\n",
            "Epoch 600/2000, Loss: 0.5771\n",
            "Epoch 700/2000, Loss: 0.5456\n",
            "Epoch 800/2000, Loss: 0.5171\n",
            "Epoch 900/2000, Loss: 0.4912\n",
            "Epoch 1000/2000, Loss: 0.4675\n",
            "Epoch 1100/2000, Loss: 0.4457\n",
            "Epoch 1200/2000, Loss: 0.4256\n",
            "Epoch 1300/2000, Loss: 0.4071\n",
            "Epoch 1400/2000, Loss: 0.3899\n",
            "Epoch 1500/2000, Loss: 0.3740\n",
            "Epoch 1600/2000, Loss: 0.3591\n",
            "Epoch 1700/2000, Loss: 0.3453\n",
            "Epoch 1800/2000, Loss: 0.3323\n",
            "Epoch 1900/2000, Loss: 0.3202\n",
            "Epoch 2000/2000, Loss: 0.3089\n",
            "Accuracy: 57.89%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}